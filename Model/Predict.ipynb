{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6110,
     "status": "ok",
     "timestamp": 1600856220045,
     "user": {
      "displayName": "PRASANTH BATTULA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GghTWEhEHBlOXjA4KhHuYud3OUFi5-ktgV9Bs91nlI=s64",
      "userId": "10508608682258355048"
     },
     "user_tz": -330
    },
    "id": "CXmM9JFSHR9v"
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1019,
     "status": "ok",
     "timestamp": 1600856291415,
     "user": {
      "displayName": "PRASANTH BATTULA",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GghTWEhEHBlOXjA4KhHuYud3OUFi5-ktgV9Bs91nlI=s64",
      "userId": "10508608682258355048"
     },
     "user_tz": -330
    },
    "id": "VRBUaucfHR-0"
   },
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.cnn_layers = Sequential(\n",
    "            # defining a 2d convolutions layer \n",
    "            Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            Dropout(0.2),\n",
    "            # defining another 2d convolutional layer\n",
    "            Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            Dropout(0.2)\n",
    "\n",
    "        )\n",
    "        \n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(4*7*7, 10)\n",
    "        )\n",
    "    \n",
    "    # defining the forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "PATH = 'saved_model.pt'\n",
    "model.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(img_path):\n",
    "    \n",
    "    # categories\n",
    "    labels = {\"0\":\"T-shirt/top\",\n",
    "    \"1\":\"Trouser\",\n",
    "    \"2\":\"Pullover\",\n",
    "    \"3\":\"Dress\",\n",
    "    \"4\":\"Coat\",\n",
    "    \"5\":\"Sandal\",\n",
    "    \"6\":\"Shirt\",\n",
    "    \"7\":\"Sneaker\",\n",
    "    \"8\":\"Bag\",\n",
    "    \"9\":\"Ankle boot\"}\n",
    "    \n",
    "    # preprocess image to resize image to 28 X 28 pixels and convert to a 2D tensor\n",
    "    p = transforms.Compose([transforms.Scale((28,28))])\n",
    "    img = Image.open(img_path)\n",
    "    img = p(img)\n",
    "    \n",
    "    pre_process = transforms.Compose(\n",
    "    [transforms.Grayscale(num_output_channels=1), transforms.ToTensor()])\n",
    "    \n",
    "    img = pre_process(img)\n",
    "    img = torch.reshape(img, (list(img.shape)[1], list(img.shape)[2]))\n",
    "    \n",
    "    # convert 2D tensor to 4D tensor\n",
    "    img_ten = torch.reshape(img, [1,1,28,28])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      output = model(img_ten.float())\n",
    "\n",
    "    softmax = torch.exp(output)\n",
    "    prob = list(softmax.numpy())\n",
    "    prediction = np.argmax(prob, axis=1)\n",
    "    print(prediction)\n",
    "    print(labels[str(prediction[0])])\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    return labels[str(prediction[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\n",
      "Bag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasanth/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:256: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labe = predict_label('test.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bag'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
